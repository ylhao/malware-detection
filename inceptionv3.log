Using TensorFlow backend.
Found 17002 images belonging to 2 classes.
Found 3751 images belonging to 2 classes.
2018-12-05 02:43:38.296390: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-05 02:43:39.520055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-12-05 02:43:39.680646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:83:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-12-05 02:43:39.682039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
2018-12-05 02:43:40.653474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-05 02:43:40.654023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
2018-12-05 02:43:40.654357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N N
2018-12-05 02:43:40.654679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   N N
2018-12-05 02:43:40.655859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10407 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-12-05 02:43:40.905815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10407 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)
1: input_1
2: lambda_1
3: conv2d_1
4: batch_normalization_1
5: activation_1
6: conv2d_2
7: batch_normalization_2
8: activation_2
9: conv2d_3
10: batch_normalization_3
11: activation_3
12: max_pooling2d_1
13: conv2d_4
14: batch_normalization_4
15: activation_4
16: conv2d_5
17: batch_normalization_5
18: activation_5
19: max_pooling2d_2
20: conv2d_9
21: batch_normalization_9
22: activation_9
23: conv2d_7
24: conv2d_10
25: batch_normalization_7
26: batch_normalization_10
27: activation_7
28: activation_10
29: average_pooling2d_1
30: conv2d_6
31: conv2d_8
32: conv2d_11
33: conv2d_12
34: batch_normalization_6
35: batch_normalization_8
36: batch_normalization_11
37: batch_normalization_12
38: activation_6
39: activation_8
40: activation_11
41: activation_12
42: mixed0
43: conv2d_16
44: batch_normalization_16
45: activation_16
46: conv2d_14
47: conv2d_17
48: batch_normalization_14
49: batch_normalization_17
50: activation_14
51: activation_17
52: average_pooling2d_2
53: conv2d_13
54: conv2d_15
55: conv2d_18
56: conv2d_19
57: batch_normalization_13
58: batch_normalization_15
59: batch_normalization_18
60: batch_normalization_19
61: activation_13
62: activation_15
63: activation_18
64: activation_19
65: mixed1
66: conv2d_23
67: batch_normalization_23
68: activation_23
69: conv2d_21
70: conv2d_24
71: batch_normalization_21
72: batch_normalization_24
73: activation_21
74: activation_24
75: average_pooling2d_3
76: conv2d_20
77: conv2d_22
78: conv2d_25
79: conv2d_26
80: batch_normalization_20
81: batch_normalization_22
82: batch_normalization_25
83: batch_normalization_26
84: activation_20
85: activation_22
86: activation_25
87: activation_26
88: mixed2
89: conv2d_28
90: batch_normalization_28
91: activation_28
92: conv2d_29
93: batch_normalization_29
94: activation_29
95: conv2d_27
96: conv2d_30
97: batch_normalization_27
98: batch_normalization_30
99: activation_27
100: activation_30
101: max_pooling2d_3
102: mixed3
103: conv2d_35
104: batch_normalization_35
105: activation_35
106: conv2d_36
107: batch_normalization_36
108: activation_36
109: conv2d_32
110: conv2d_37
111: batch_normalization_32
112: batch_normalization_37
113: activation_32
114: activation_37
115: conv2d_33
116: conv2d_38
117: batch_normalization_33
118: batch_normalization_38
119: activation_33
120: activation_38
121: average_pooling2d_4
122: conv2d_31
123: conv2d_34
124: conv2d_39
125: conv2d_40
126: batch_normalization_31
127: batch_normalization_34
128: batch_normalization_39
129: batch_normalization_40
130: activation_31
131: activation_34
132: activation_39
133: activation_40
134: mixed4
135: conv2d_45
136: batch_normalization_45
137: activation_45
138: conv2d_46
139: batch_normalization_46
140: activation_46
141: conv2d_42
142: conv2d_47
143: batch_normalization_42
144: batch_normalization_47
145: activation_42
146: activation_47
147: conv2d_43
148: conv2d_48
149: batch_normalization_43
150: batch_normalization_48
151: activation_43
152: activation_48
153: average_pooling2d_5
154: conv2d_41
155: conv2d_44
156: conv2d_49
157: conv2d_50
158: batch_normalization_41
159: batch_normalization_44
160: batch_normalization_49
161: batch_normalization_50
162: activation_41
163: activation_44
164: activation_49
165: activation_50
166: mixed5
167: conv2d_55
168: batch_normalization_55
169: activation_55
170: conv2d_56
171: batch_normalization_56
172: activation_56
173: conv2d_52
174: conv2d_57
175: batch_normalization_52
176: batch_normalization_57
177: activation_52
178: activation_57
179: conv2d_53
180: conv2d_58
181: batch_normalization_53
182: batch_normalization_58
183: activation_53
184: activation_58
185: average_pooling2d_6
186: conv2d_51
187: conv2d_54
188: conv2d_59
189: conv2d_60
190: batch_normalization_51
191: batch_normalization_54
192: batch_normalization_59
193: batch_normalization_60
194: activation_51
195: activation_54
196: activation_59
197: activation_60
198: mixed6
199: conv2d_65
200: batch_normalization_65
201: activation_65
202: conv2d_66
203: batch_normalization_66
204: activation_66
205: conv2d_62
206: conv2d_67
207: batch_normalization_62
208: batch_normalization_67
209: activation_62
210: activation_67
211: conv2d_63
212: conv2d_68
213: batch_normalization_63
214: batch_normalization_68
215: activation_63
216: activation_68
217: average_pooling2d_7
218: conv2d_61
219: conv2d_64
220: conv2d_69
221: conv2d_70
222: batch_normalization_61
223: batch_normalization_64
224: batch_normalization_69
225: batch_normalization_70
226: activation_61
227: activation_64
228: activation_69
229: activation_70
230: mixed7
231: conv2d_73
232: batch_normalization_73
233: activation_73
234: conv2d_74
235: batch_normalization_74
236: activation_74
237: conv2d_71
238: conv2d_75
239: batch_normalization_71
240: batch_normalization_75
241: activation_71
242: activation_75
243: conv2d_72
244: conv2d_76
245: batch_normalization_72
246: batch_normalization_76
247: activation_72
248: activation_76
249: max_pooling2d_4
250: mixed8
251: conv2d_81
252: batch_normalization_81
253: activation_81
254: conv2d_78
255: conv2d_82
256: batch_normalization_78
257: batch_normalization_82
258: activation_78
259: activation_82
260: conv2d_79
261: conv2d_80
262: conv2d_83
263: conv2d_84
264: average_pooling2d_8
265: conv2d_77
266: batch_normalization_79
267: batch_normalization_80
268: batch_normalization_83
269: batch_normalization_84
270: conv2d_85
271: batch_normalization_77
272: activation_79
273: activation_80
274: activation_83
275: activation_84
276: batch_normalization_85
277: activation_77
278: mixed9_0
279: concatenate_1
280: activation_85
281: mixed9
282: conv2d_90
283: batch_normalization_90
284: activation_90
285: conv2d_87
286: conv2d_91
287: batch_normalization_87
288: batch_normalization_91
289: activation_87
290: activation_91
291: conv2d_88
292: conv2d_89
293: conv2d_92
294: conv2d_93
295: average_pooling2d_9
296: conv2d_86
297: batch_normalization_88
298: batch_normalization_89
299: batch_normalization_92
300: batch_normalization_93
301: conv2d_94
302: batch_normalization_86
303: activation_88
304: activation_89
305: activation_92
306: activation_93
307: batch_normalization_94
308: activation_86
309: mixed9_1
310: concatenate_2
311: activation_94
312: mixed10
313: my_global_average_pooling_layer_1
314: my_dropout_layer_1
315: my_dense_layer_1
Epoch 1/10
531/531 [==============================] - 440s 830ms/step - loss: 0.2223 - acc: 0.9138 - val_loss: 0                                       .3322 - val_acc: 0.8694

Epoch 00001: val_loss improved from inf to 0.33216, saving model to ./model/inceptionv3-finetune.hdf5
Epoch 2/10
531/531 [==============================] - 417s 786ms/step - loss: 0.1154 - acc: 0.9570 - val_loss: 0                                       .6965 - val_acc: 0.6725

Epoch 00002: val_loss did not improve from 0.33216
Epoch 3/10
531/531 [==============================] - 415s 781ms/step - loss: 0.0745 - acc: 0.9736 - val_loss: 1                                       .7143 - val_acc: 0.5590

Epoch 00003: val_loss did not improve from 0.33216
Epoch 4/10
531/531 [==============================] - 414s 780ms/step - loss: 0.0562 - acc: 0.9790 - val_loss: 0                                       .4210 - val_acc: 0.8271

